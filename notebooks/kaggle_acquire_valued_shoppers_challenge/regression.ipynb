{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RoRxBv3bRjy"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2019 The Lifetime Value Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tkQUXmWhqRY"
      },
      "source": [
        "# Lifetime Value prediction for Kaggle Acquire Valued Customer Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw8bm9nV6YJ5"
      },
      "source": [
        "\u003ctable align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/google/lifetime_value/blob/master/notebooks/kaggle_acquire_valued_shoppers_challenge/regression.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/google/lifetime_value/blob/master/notebooks/kaggle_acquire_valued_shoppers_challenge/regression.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KObdQwyXH2mC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_probability as tfp\n",
        "import tqdm\n",
        "from typing import Sequence\n",
        "\n",
        "# install and import ltv\n",
        "!pip install -q git+https://github.com/google/lifetime_value\n",
        "import lifetime_value as ltv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K41RmAfNXtu_"
      },
      "outputs": [],
      "source": [
        "tfd = tfp.distributions\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set_style('whitegrid')\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoN-PRvNuIti"
      },
      "source": [
        "## Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GGpDbxd3S5L"
      },
      "outputs": [],
      "source": [
        "COMPANY = '103600030'  # @param { isTemplate: true, type: 'string'}\n",
        "LOSS = 'ziln'  # @param { isTemplate: true, type: 'string'} ['mse', 'ziln']\n",
        "MODEL = 'dnn'  # @param { isTemplate: true, type: 'string'} ['linear', 'dnn']\n",
        "LEARNING_RATE = 0.0002  # @param { isTemplate: true}\n",
        "EPOCHS = 400  # @param { isTemplate: true, type: 'integer'}\n",
        "OUTPUT_CSV_FOLDER = '/tmp/lifetime-value/kaggle_acquire_valued_shoppers_challenge/result'  # @param { isTemplate: true, type: 'string'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK9Y5NoMtm3X"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_FEATURES = ['chain', 'dept', 'category', 'brand', 'productmeasure']\n",
        "NUMERIC_FEATURES = ['log_calibration_value']\n",
        "\n",
        "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTaK6fFXMWT"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFi0JMPu138h"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krXMbrkVNtdN"
      },
      "source": [
        "Setup kaggle API correctly following https://www.kaggle.com/docs/api\n",
        "```\n",
        "%%shell\n",
        "mkdir ~/.kaggle\n",
        "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} \u003e ~/.kaggle/kaggle.json\n",
        "pip install kaggle\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gf4ipd-14x0"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "if [ -e /tmp/lifetime-value/acquire-valued-shoppers-challenge/transactions.csv ]\n",
        "then\n",
        "  echo \"File already exists, no need to download.\"\n",
        "else\n",
        "  rm -rf /tmp/lifetime-value/acquire-valued-shoppers-challenge\n",
        "  mkdir -p /tmp/lifetime-value/acquire-valued-shoppers-challenge\n",
        "  cd /tmp/lifetime-value/acquire-valued-shoppers-challenge\n",
        "  kaggle competitions download -c acquire-valued-shoppers-challenge\n",
        "  echo \"Unzip file. This may take 10 min.\"\n",
        "  gunzip transactions.csv.gz\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT53azGsa2a2"
      },
      "source": [
        "### Load transaction csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tIMvE3dW1Ky"
      },
      "outputs": [],
      "source": [
        "def load_transaction_data(company):\n",
        "  all_data_filename = '/tmp/lifetime-value/acquire-valued-shoppers-challenge/transactions.csv'\n",
        "  one_company_data_filename = (\n",
        "      '/tmp/lifetime-value/acquire-valued-shoppers-challenge/transactions_company_{}.csv'\n",
        "      .format(COMPANY))\n",
        "  if os.path.isfile(one_company_data_filename):\n",
        "    df = pd.read_csv(one_company_data_filename)\n",
        "  else:\n",
        "    data_list = []\n",
        "    chunksize = 10**6\n",
        "    # 350 iterations\n",
        "    for chunk in tqdm.tqdm(pd.read_csv(all_data_filename, chunksize=chunksize)):\n",
        "      data_list.append(chunk.query(\"company=='{}'\".format(company)))\n",
        "    df = pd.concat(data_list, axis=0)\n",
        "    df.to_csv(one_company_data_filename, index=None)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ra4bfwCVwKn"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlJl5g9Delmi"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "  df = df.query('purchaseamount\u003e0')\n",
        "  df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
        "  df['start_date'] = df.groupby('id')['date'].transform('min')\n",
        "\n",
        "  # Compute calibration values\n",
        "  calibration_value = (\n",
        "      df.query('date==start_date').groupby('id')\n",
        "      ['purchaseamount'].sum().reset_index())\n",
        "  calibration_value.columns = ['id', 'calibration_value']\n",
        "\n",
        "  # Compute holdout values\n",
        "  one_year_holdout_window_mask = (\n",
        "      (df['date'] \u003e df['start_date']) \u0026\n",
        "      (df['date'] \u003c= df['start_date'] + np.timedelta64(365, 'D')))\n",
        "  holdout_value = (\n",
        "      df[one_year_holdout_window_mask].groupby('id')\n",
        "      ['purchaseamount'].sum().reset_index())\n",
        "  holdout_value.columns = ['id', 'holdout_value']\n",
        "\n",
        "  # Compute calibration attributes\n",
        "  calibration_attributes = (\n",
        "      df.query('date==start_date').sort_values(\n",
        "          'purchaseamount', ascending=False).groupby('id')[[\n",
        "              'chain', 'dept', 'category', 'brand', 'productmeasure'\n",
        "          ]].first().reset_index())\n",
        "\n",
        "  # Merge dataframes\n",
        "  customer_level_data = (\n",
        "      calibration_value.merge(calibration_attributes, how='left',\n",
        "                              on='id').merge(\n",
        "                                  holdout_value, how='left', on='id'))\n",
        "  customer_level_data['holdout_value'] = (\n",
        "      customer_level_data['holdout_value'].fillna(0.))\n",
        "  customer_level_data[CATEGORICAL_FEATURES] = (\n",
        "      customer_level_data[CATEGORICAL_FEATURES].fillna('UNKNOWN'))\n",
        "\n",
        "  # Specify data types\n",
        "  customer_level_data['log_calibration_value'] = (\n",
        "      np.log(customer_level_data['calibration_value']).astype('float32'))\n",
        "  customer_level_data['chain'] = (\n",
        "      customer_level_data['chain'].astype('category'))\n",
        "  customer_level_data['dept'] = (customer_level_data['dept'].astype('category'))\n",
        "  customer_level_data['brand'] = (\n",
        "      customer_level_data['brand'].astype('category'))\n",
        "  customer_level_data['category'] = (\n",
        "      customer_level_data['category'].astype('category'))\n",
        "  customer_level_data['label'] = (\n",
        "      customer_level_data['holdout_value'].astype('float32'))\n",
        "  return customer_level_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP3q6uuMoXhA"
      },
      "source": [
        "### Load customer-level csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8B4zV1xoeMX"
      },
      "outputs": [],
      "source": [
        "def load_customer_level_csv(company):\n",
        "  customer_level_data_file = (\n",
        "      '/tmp/lifetime-value/acquire-valued-shoppers-challenge/customer_level_data_company_{}.csv'\n",
        "      .format(company))\n",
        "  if os.path.isfile(customer_level_data_file):\n",
        "    customer_level_data = pd.read_csv(customer_level_data_file)\n",
        "  else:\n",
        "    customer_level_data = preprocess(load_transaction_data(company))\n",
        "  for cat_col in CATEGORICAL_FEATURES:\n",
        "    customer_level_data[cat_col] = (\n",
        "        customer_level_data[cat_col].astype('category'))\n",
        "  for num_col in [\n",
        "      'log_calibration_value', 'calibration_value', 'holdout_value'\n",
        "  ]:\n",
        "    customer_level_data[num_col] = (\n",
        "        customer_level_data[num_col].astype('float32'))\n",
        "\n",
        "  return customer_level_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88dVPdt5QWpu"
      },
      "outputs": [],
      "source": [
        "# Processes data. 350 iteration in total. May take 10min.\n",
        "customer_level_data = load_customer_level_csv(COMPANY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09tqgvANtsil"
      },
      "source": [
        "We observe a mixture of zero and lognormal distribution of holdout value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtF0z3VbmGev"
      },
      "outputs": [],
      "source": [
        "customer_level_data.label.apply(np.log1p).hist(bins=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4kN0uk4kZ68"
      },
      "source": [
        "### Make train/eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc0MLKx2yD72"
      },
      "outputs": [],
      "source": [
        "def linear_split(df):\n",
        "  # get_dummies preserves numeric features.\n",
        "  x = pd.get_dummies(df[ALL_FEATURES], drop_first=True).astype('float32').values\n",
        "  y = df['label'].values\n",
        "  y0 = df['calibration_value'].values\n",
        "\n",
        "  x_train, x_eval, y_train, y_eval, y0_train, y0_eval = (\n",
        "      model_selection.train_test_split(\n",
        "          x, y, y0, test_size=0.2, random_state=123))\n",
        "\n",
        "  return x_train, x_eval, y_train, y_eval, y0_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAGbXp9ax042"
      },
      "outputs": [],
      "source": [
        "def dnn_split(df):\n",
        "  for key in CATEGORICAL_FEATURES:\n",
        "    encoder = preprocessing.LabelEncoder()\n",
        "    df[key] = encoder.fit_transform(df[key])\n",
        "\n",
        "  y0 = df['calibration_value'].values\n",
        "  df_train, df_eval, y0_train, y0_eval = model_selection.train_test_split(\n",
        "      df, y0, test_size=0.2, random_state=123)\n",
        "\n",
        "  def feature_dict(df):\n",
        "    features = {k: v.values for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
        "    features['numeric'] = df[NUMERIC_FEATURES].values\n",
        "    return features\n",
        "\n",
        "  x_train, y_train = feature_dict(df_train), df_train['label'].values\n",
        "  x_eval, y_eval = feature_dict(df_eval), df_eval['label'].values\n",
        "\n",
        "  return x_train, x_eval, y_train, y_eval, y0_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqbShWBzR4NE"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAOttr0W0yTM"
      },
      "outputs": [],
      "source": [
        "def linear_model(output_units):\n",
        "  return tf.keras.experimental.LinearModel(output_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7huREFbR7Dl"
      },
      "outputs": [],
      "source": [
        "def embedding_dim(x):\n",
        "  return int(x**.25) + 1\n",
        "\n",
        "\n",
        "def embedding_layer(vocab_size):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(\n",
        "          input_dim=vocab_size,\n",
        "          output_dim=embedding_dim(vocab_size),\n",
        "          input_length=1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "  ])\n",
        "\n",
        "\n",
        "def dnn_model(output_units, df):\n",
        "  numeric_input = tf.keras.layers.Input(\n",
        "      shape=(len(NUMERIC_FEATURES),), name='numeric')\n",
        "\n",
        "  embedding_inputs = [\n",
        "      tf.keras.layers.Input(shape=(1,), name=key, dtype=np.int64)\n",
        "      for key in CATEGORICAL_FEATURES\n",
        "  ]\n",
        "\n",
        "  embedding_outputs = [\n",
        "      embedding_layer(vocab_size=df[key].nunique())(input)\n",
        "      for key, input in zip(CATEGORICAL_FEATURES, embedding_inputs)\n",
        "  ]\n",
        "\n",
        "  deep_input = tf.keras.layers.concatenate([numeric_input] + embedding_outputs)\n",
        "  deep_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(output_units),\n",
        "  ])\n",
        "  return tf.keras.Model(\n",
        "      inputs=[numeric_input] + embedding_inputs, outputs=deep_model(deep_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8l-KzZ12fbK"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3HzXsj61uy3"
      },
      "outputs": [],
      "source": [
        "if LOSS == 'mse':\n",
        "  loss = keras.losses.MeanSquaredError()\n",
        "  output_units = 1\n",
        "\n",
        "if LOSS == 'ziln':\n",
        "  loss = ltv.zero_inflated_lognormal_loss\n",
        "  output_units = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pNM4q5m19Dv"
      },
      "outputs": [],
      "source": [
        "if MODEL == 'linear':\n",
        "  x_train, x_eval, y_train, y_eval, y0_eval = linear_split(customer_level_data)\n",
        "  model = linear_model(output_units)\n",
        "\n",
        "if MODEL == 'dnn':\n",
        "  x_train, x_eval, y_train, y_eval, y0_eval = dnn_split(customer_level_data)\n",
        "  model = dnn_model(output_units, customer_level_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un-yJPHp31gp"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=loss, optimizer=keras.optimizers.Adam(lr=LEARNING_RATE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GQ-RlIAfT62"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_lr=1e-6),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BjnHV7MWhK1"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=1024,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=2,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=(x_eval, y_eval)).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAJGs5SebDeN"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(history)[['loss', 'val_loss']][2:].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHaiutmy2aYm"
      },
      "source": [
        "### Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6E_5gYAYQMw"
      },
      "outputs": [],
      "source": [
        "if LOSS == 'mse':\n",
        "  y_pred = model.predict(x=x_eval, batch_size=1024).flatten()\n",
        "\n",
        "if LOSS == 'ziln':\n",
        "  logits = model.predict(x=x_eval, batch_size=1024)\n",
        "  y_pred = ltv.zero_inflated_lognormal_pred(logits).numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm28qKSGXNyr"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame({\n",
        "    'y_true': y_eval,\n",
        "    'y_pred': y_pred,\n",
        "})\n",
        "df_pred.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zROhsEWxnA5u"
      },
      "source": [
        "### Gini Coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRsJ7y-632h_"
      },
      "outputs": [],
      "source": [
        "gain = pd.DataFrame({\n",
        "    'lorenz': ltv.cumulative_true(y_eval, y_eval),\n",
        "    'baseline': ltv.cumulative_true(y_eval, y0_eval),\n",
        "    'model': ltv.cumulative_true(y_eval, y_pred),\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg-ndbve4AL_"
      },
      "outputs": [],
      "source": [
        "num_customers = np.float32(gain.shape[0])\n",
        "gain['cumulative_customer'] = (np.arange(num_customers) + 1.) / num_customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEoAvuCj4OVy"
      },
      "outputs": [],
      "source": [
        "ax = gain[[\n",
        "    'cumulative_customer',\n",
        "    'lorenz',\n",
        "    'baseline',\n",
        "    'model',\n",
        "]].plot(\n",
        "    x='cumulative_customer', figsize=(8, 5), legend=True)\n",
        "\n",
        "ax.legend(['Groundtruth', 'Baseline', 'Model'], loc='upper left')\n",
        "\n",
        "ax.set_xlabel('Cumulative Fraction of Customers')\n",
        "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
        "ax.set_xlim((0, 1.))\n",
        "\n",
        "ax.set_ylabel('Cumulative Fraction of Total Lifetime Value')\n",
        "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
        "ax.set_ylim((0, 1.05))\n",
        "ax.set_title('Gain Chart')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzPqaiNO4iWC"
      },
      "outputs": [],
      "source": [
        "gini = ltv.gini_from_gain(gain[['lorenz', 'baseline', 'model']])\n",
        "gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S84RitIa9PBu"
      },
      "source": [
        "### Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7sKbsEf6RvF"
      },
      "outputs": [],
      "source": [
        "df_decile = ltv.decile_stats(y_eval, y_pred)\n",
        "df_decile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHdLqUqdL4hf"
      },
      "outputs": [],
      "source": [
        "ax = df_decile[['label_mean', 'pred_mean']].plot.bar(rot=0)\n",
        "\n",
        "ax.set_title('Decile Chart')\n",
        "ax.set_xlabel('Prediction bucket')\n",
        "ax.set_ylabel('Average bucket value')\n",
        "ax.legend(['Label', 'Prediction'], loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK6DQ89xU-d4"
      },
      "source": [
        "### Rank Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9qWGyY3WePz"
      },
      "outputs": [],
      "source": [
        "def spearmanr(x1: Sequence[float], x2: Sequence[float]) -\u003e float:\n",
        "  \"\"\"Calculates spearmanr rank correlation coefficient.\n",
        "\n",
        "  See https://docs.scipy.org/doc/scipy/reference/stats.html.\n",
        "\n",
        "  Args:\n",
        "    x1: 1D array_like.\n",
        "    x2: 1D array_like.\n",
        "\n",
        "  Returns:\n",
        "    correlation: float.\n",
        "  \"\"\"\n",
        "  return stats.spearmanr(x1, x2, nan_policy='raise')[0]\n",
        "\n",
        "\n",
        "spearman_corr = spearmanr(y_eval, y_pred)\n",
        "spearman_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i_AbqhXcurk"
      },
      "source": [
        "### All metrics together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umqg1-0Bc1HS"
      },
      "outputs": [],
      "source": [
        "df_metrics = pd.DataFrame(\n",
        "    {\n",
        "        'company': COMPANY,\n",
        "        'model': MODEL,\n",
        "        'loss': LOSS,\n",
        "        'label_mean': y_eval.mean(),\n",
        "        'pred_mean': y_pred.mean(),\n",
        "        'label_positive': np.mean(y_eval \u003e 0),\n",
        "        'decile_mape': df_decile['decile_mape'].mean(),\n",
        "        'baseline_gini': gini['normalized'][1],\n",
        "        'gini': gini['normalized'][2],\n",
        "        'spearman_corr': spearman_corr,\n",
        "    },\n",
        "    index=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LV1Hs3xcxnd"
      },
      "outputs": [],
      "source": [
        "df_metrics[[\n",
        "    'company',\n",
        "    'model',\n",
        "    'loss',\n",
        "    'label_mean',\n",
        "    'pred_mean',\n",
        "    'label_positive',\n",
        "    'decile_mape',\n",
        "    'baseline_gini',\n",
        "    'gini',\n",
        "    'spearman_corr',\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVy6lYn4mSrj"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtkQ4mqUEFsb"
      },
      "outputs": [],
      "source": [
        "output_path = os.path.join(OUTPUT_CSV_FOLDER, COMPANY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qmLzJqOEFsm"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(output_path):\n",
        "  os.makedirs(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61B5Zc_UEFsr"
      },
      "outputs": [],
      "source": [
        "output_file = os.path.join(output_path,\n",
        "                           '{}_regression_{}.csv'.format(MODEL, LOSS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqglbXfwEFsv"
      },
      "outputs": [],
      "source": [
        "df_metrics.to_csv(output_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
